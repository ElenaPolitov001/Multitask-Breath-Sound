{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "12_multitask_breath.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Get Google Drive"
      ],
      "metadata": {
        "id": "EiWO3Bg6rS_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Get Google Drive\r\n",
        "%load_ext autoreload\r\n",
        "from google.colab import drive\r\n",
        "import os\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "path = '/content/gdrive/My Drive/Breath-Data/data/training/'\r\n",
        "os.chdir(path)\r\n",
        "!ls"
      ],
      "outputs": [],
      "metadata": {
        "id": "7f8Al_1CrJ8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98dca2ed-0723-41ff-8323-44ecfc0e4197"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from glob import glob\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Bronchiolitis/*')\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "2k_7W88jqJr2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install pydub"
      ],
      "outputs": [],
      "metadata": {
        "id": "pIvtYXXL6i2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1733a5-6a4d-429a-9342-3704cf1c4944"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create data breath"
      ],
      "metadata": {
        "id": "Zj5mkd12vm2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import random\r\n",
        "\r\n",
        "from pydub import AudioSegment\r\n",
        "from pydub import AudioSegment, effects  \r\n",
        "from rnnoise_wrapper import RNNoise\r\n",
        "\r\n",
        "denoiser = RNNoise()\r\n",
        "\r\n",
        "print(denoiser)\r\n",
        "\r\n",
        "duration_breath = 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "NKddBVZEvlm3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_audio_segment (filename, source_path, destination_path, start, end, status, i):\r\n",
        "    \"\"\"[summary]\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "        filename {[type]} -- [filename of the audio file]\r\n",
        "        source_path {[type]} -- [source of a audio file]\r\n",
        "        destination_path {[type]} -- [destination of a output file]\r\n",
        "        start {[type]} -- [description]\r\n",
        "        end {[type]} -- [description]\r\n",
        "        status {[type]} -- [type of breath]\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Pydub works in milliseconds\r\n",
        "    start = start * 1000 \r\n",
        "    end = end * 1000\r\n",
        "    offset_time = (end - start)\r\n",
        "\r\n",
        "    global duration_breath\r\n",
        "    # print(offset_time)\r\n",
        "    print(duration_breath)\r\n",
        "\r\n",
        "    if offset_time > duration_breath and offset_time < 4000:\r\n",
        "      duration_breath = offset_time\r\n",
        "    \r\n",
        "    # print(start, end, end_2)\r\n",
        "    \r\n",
        "    # Get the audio file\r\n",
        "    src_Audio = AudioSegment.from_wav(source_path)\r\n",
        "    \r\n",
        "    # Get the audio offset\r\n",
        "    offset_Audio = AudioSegment.from_wav('/content/gdrive/My Drive/Breath-Data/Sine.wav')\r\n",
        "    \r\n",
        "    #Cut the right part\r\n",
        "    output_audio = src_Audio[start:end]\r\n",
        "\r\n",
        "    try:\r\n",
        "      if offset_time > 4000:\r\n",
        "        raise Exception(\"offset_time over 4000\")\r\n",
        "\r\n",
        "      # random start audio\r\n",
        "      rand = random.randint(0, int(4000 - offset_time))\r\n",
        "    \r\n",
        "    \r\n",
        "      #Convert to 5s\r\n",
        "      # output_audio = offset_Audio[0:rand] + output_audio[0:offset_time] + offset_Audio[rand+offset_time+1:4000]\r\n",
        "\r\n",
        "      ###########################################################################\r\n",
        "      for j in range(int(end - 4000), int(start), 100):\r\n",
        "        if j < 0:\r\n",
        "          continue\r\n",
        "        output_audio = src_Audio[j:j+4000]\r\n",
        "        # Define name file\r\n",
        "        fname =  filename + \"_\" + str(i) + \"_\" + str(j) + \"_\" + status + '.wav'\r\n",
        "        # print(destination_path)\r\n",
        "\r\n",
        "        # normalized sound\r\n",
        "        normalizedsound = effects.normalize(output_audio)\r\n",
        "        # denoised_audio = denoiser.filter(normalizedsound, sample_rate=8000, voice_prob_threshold=0.0, save_source_sample_rate=True)\r\n",
        "\r\n",
        "        # denoiser.write_wav(destination_path + '/' + fname, denoised_audio)\r\n",
        "        normalizedsound.export(destination_path + '/' + fname, \r\n",
        "                            format=\"wav\")  # Exports to a wav file in the current path.\r\n",
        "      ###########################################################################\r\n",
        "      \r\n",
        "      # Define name file\r\n",
        "      # fname =  filename + \"_\" + str(i) + \"_\" + status + '.wav'\r\n",
        "      # print(destination_path)\r\n",
        "      # output_audio.export(destination_path + '/' + fname, \r\n",
        "      #                     format=\"wav\")  # Exports to a wav file in the current path.\r\n",
        "    except Exception as e:\r\n",
        "      print(e)\r\n",
        "      pass"
      ],
      "outputs": [],
      "metadata": {
        "id": "1_724qB9wp0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "time_normal = 0\r\n",
        "time_deep = 0\r\n",
        "count_normal = 0\r\n",
        "count_deep = 0\r\n",
        "\r\n",
        "\r\n",
        "def split_by_label(source_path, destination_path, label_path, output_folder):\r\n",
        "\r\n",
        "    # Check the output directory status \r\n",
        "\r\n",
        "    check_directory(destination_path, output_folder)\r\n",
        "\r\n",
        "    output_path = os.path.join(destination_path, output_folder) + '/'\r\n",
        "    # Get all the audio files\r\n",
        "    filenames = os.listdir(source_path)\r\n",
        "    \r\n",
        "    meta_data=[[\"\",\"\"]]\r\n",
        "    \r\n",
        "    # Go through all the file \r\n",
        "    for filename in filenames:\r\n",
        "        \r\n",
        "        # take the file name without dot\r\n",
        "        filename =  filename.split(\".\")[0]\r\n",
        "        \r\n",
        "        #get wav file name path\r\n",
        "        wav_path = source_path + filename + \".wav\"\r\n",
        "        \r\n",
        "        #get label filename path\r\n",
        "        csv_path = label_path + filename + \".txt\"\r\n",
        "        \r\n",
        "        #read the label file path \r\n",
        "#         label = pd.read_csv(csv_path, delim_whitespace= True)\r\n",
        "\r\n",
        "        if not os.path.isfile(csv_path):\r\n",
        "            print(\"Not found:\", filename)\r\n",
        "            continue\r\n",
        "\r\n",
        "        # Open the file with read only permit\r\n",
        "        label = open(csv_path, \"r\")\r\n",
        "        # use readlines to read all lines in the file\r\n",
        "        # The variable \"lines\" is a list containing all lines in the file\r\n",
        "        lines = label.readlines()\r\n",
        "        \r\n",
        "#         # split the text\r\n",
        "#         words = text.split()\r\n",
        "        \r\n",
        "        # close the file after reading the lines.\r\n",
        "        label.close()\r\n",
        "        \r\n",
        "        print(csv_path)\r\n",
        "        \r\n",
        "        i = 0\r\n",
        "        \r\n",
        "        #Normal breath\r\n",
        "        for line in lines:\r\n",
        "            breath_start = float(line.split()[0])\r\n",
        "            breath_end   = float(line.split()[1])\r\n",
        "            try:\r\n",
        "                breath = line.split()[2]\r\n",
        "                if breath == 'strong':\r\n",
        "                  breath = 'heavy'\r\n",
        "                if breath == 'normla':\r\n",
        "                  breath = 'normal'\r\n",
        "            except Exception as e:\r\n",
        "                print(e)\r\n",
        "                continue\r\n",
        "            \r\n",
        "            time_breath = (breath_end - breath_start)\r\n",
        "            if breath == 'normal' and time_breath > 1.73:\r\n",
        "              continue\r\n",
        "              # global time_normal\r\n",
        "              # time_normal += (breath_end - breath_start)\r\n",
        "              # global count_normal\r\n",
        "              # count_normal += 1\r\n",
        "\r\n",
        "            if breath == 'deep' and time_breath < 2.52:\r\n",
        "              continue\r\n",
        "\r\n",
        "              # global time_deep\r\n",
        "              # time_deep += (breath_end - breath_start)\r\n",
        "              # global count_deep\r\n",
        "              # count_deep += 1\r\n",
        "\r\n",
        "            output_by_label = os.path.join(output_path, breath)               \r\n",
        "            if not os.path.exists(output_by_label):\r\n",
        "                os.makedirs(output_by_label)\r\n",
        "\r\n",
        "            # check pre and nextline label:\r\n",
        "            \r\n",
        "            try:\r\n",
        "                nextline = lines[lines.index(line)+1]\r\n",
        "                breath_next = nextline.split()[2]\r\n",
        "                if breath_next == 'strong':\r\n",
        "                  breath_next = 'heavy'\r\n",
        "                if breath_next == 'normla':\r\n",
        "                  breath_next = 'normal'\r\n",
        "            except Exception as e:\r\n",
        "                print(e)\r\n",
        "                continue\r\n",
        "\r\n",
        "            try:\r\n",
        "                preline = lines[lines.index(line)+1]\r\n",
        "                breath_pre = nextline.split()[2]\r\n",
        "                if breath_pre == 'strong':\r\n",
        "                  breath_pre = 'heavy'\r\n",
        "                if breath_pre == 'normla':\r\n",
        "                  breath_pre = 'normal'\r\n",
        "            except Exception as e:\r\n",
        "                print(e)\r\n",
        "                continue\r\n",
        "\r\n",
        "            if breath != breath_pre or breath != breath_next:\r\n",
        "              continue\r\n",
        "            \r\n",
        "\r\n",
        "            #Export the file\r\n",
        "            get_audio_segment(filename, wav_path, output_by_label, breath_start, breath_end, breath, i)\r\n",
        "            \r\n",
        "            i += 1\r\n",
        "            \r\n",
        "            # Add other label\r\n",
        "            "
      ],
      "outputs": [],
      "metadata": {
        "id": "qJoCXjTswti5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Dev set\r\n",
        "DEV_INPUT_PATH = \"/content/gdrive/My Drive/Breath-Data/raw_data/training/\"\r\n",
        "DEV_OUTPUT_PATH = \"/content/data/\"\r\n",
        "DEV_LABEL_PATH = \"/content/gdrive/My Drive/Breath-Data/raw_data/training/label/\"\r\n",
        "\r\n",
        "TRAIN_OUTPUT_FOLDER = \"training\"\r\n",
        "TEST_OUTPUT_FOLDER = \"validation\"\r\n",
        "\r\n",
        "\r\n",
        "# Test set\r\n",
        "TEST_INPUT_PATH = \"/content/gdrive/My Drive/Breath-Data/raw_data/validation/\"\r\n",
        "TEST_OUTPUT_PATH = \"/content/data/\"\r\n",
        "TEST_LABEL_PATH = \"/content/gdrive/My Drive/Breath-Data/raw_data/validation/label/\"\r\n",
        "\r\n",
        "CHUNK = 5\r\n",
        "OVERLAP = 0.5\r\n",
        "\r\n",
        "LABEL_FOLDER = 'label'\r\n",
        " # Type of breath\r\n",
        "BREATH_TYPE = ['normal', 'deep', 'heavy', 'other']"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZcijxxLywuVK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.chdir(TEST_LABEL_PATH)\r\n",
        "!ls"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9k8rAZjw-Jq",
        "outputId": "9f014b3f-57b5-4c28-d557-8f547f9e50c9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def check_directory(origin_path, folder):\r\n",
        "    directory = os.path.join(origin_path, folder)\r\n",
        "    if not os.path.exists(directory):\r\n",
        "        os.makedirs(directory)"
      ],
      "outputs": [],
      "metadata": {
        "id": "JXPjim4Fw_r-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Training set\r\n",
        "split_by_label(DEV_INPUT_PATH, DEV_OUTPUT_PATH, DEV_LABEL_PATH, TRAIN_OUTPUT_FOLDER)\r\n",
        "\r\n",
        "# Validation set\r\n",
        "split_by_label(TEST_INPUT_PATH, TEST_OUTPUT_PATH, TEST_LABEL_PATH, TEST_OUTPUT_FOLDER)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUM2y2-axB0I",
        "outputId": "e36330d4-c795-4dec-f125-0350cc3db5b4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Threadhold time for normal and deep\r\n",
        "\r\n",
        "# print(time_normal/count_normal) # 1.72510334493671\r\n",
        "# print(time_deep/count_deep) # 2.5210367994858607"
      ],
      "outputs": [],
      "metadata": {
        "id": "UwjoY1oS5VHC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!cp -r \"/content/gdrive/My Drive/Breath-Data/data/training/other\" \"/content/data/training\""
      ],
      "outputs": [],
      "metadata": {
        "id": "4_RLQ4LOxx2r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!cp -r \"/content/gdrive/My Drive/Breath-Data/data/validation/other\" \"/content/data/validation\""
      ],
      "outputs": [],
      "metadata": {
        "id": "QOHV5lI9yu-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RNNoise filter"
      ],
      "metadata": {
        "id": "2Rw6U28j1-xj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!sudo apt-get install autoconf libtool"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-imuJfKfsAcD",
        "outputId": "384f2ec6-2459-44ad-bdd3-6f41c4bd82a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!git clone https://github.com/Desklop/RNNoise_Wrapper"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egemqbzqvfoG",
        "outputId": "8c7d2199-e979-41b1-fcf2-bfe2e07e176c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!cd RNNoise_Wrapper\r\n",
        "!ls\r\n",
        "!pwd"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_YSOlW7vlk8",
        "outputId": "fe6a033d-a735-456e-e9d0-c99ebe562fc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "path = '/content/gdrive/My Drive/Breath-Data/data/training/RNNoise_Wrapper'\r\n",
        "os.chdir(path)\r\n",
        "!ls"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zmZGk8MwZSO",
        "outputId": "f48cd723-d0d7-4187-f9b4-97dc50be3771"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!chmod +x ./compile_rnnoise.sh\r\n",
        "!./compile_rnnoise.sh"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D95NFEAMv65n",
        "outputId": "3bd0e9cc-0b75-42fe-dbf2-7ba9b0540d2c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from rnnoise_wrapper import RNNoise\r\n",
        "\r\n",
        "denoiser = RNNoise()\r\n",
        "\r\n",
        "audio = denoiser.read_wav('/content/data/training/deep/04_female_21_LAnh_10_deep.wav')\r\n",
        "denoised_audio = denoiser.filter(audio, sample_rate=8000, voice_prob_threshold=0.0, save_source_sample_rate=True)\r\n",
        "denoiser.write_wav('/content/data/training/test_deep_denoised.wav', denoised_audio)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RNWoO3YpxGFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5af3d1be-792b-4ea3-993e-de2b7ab4228e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from glob import glob\r\n",
        "list_breath_path = glob('/content/data/training/deep/*')\r\n",
        "\r\n",
        "print(len(list_breath_path))\r\n",
        "print(list_breath_path[0])\r\n",
        "\r\n",
        "audio_file = AudioSegment.from_wav(list_breath_path[0])\r\n",
        "\r\n",
        "print(len(audio_file))\r\n",
        "print(audio_file)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJTFmvRb5PQH",
        "outputId": "49d2fa1f-e81e-49f7-b98e-89215a53155d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from __future__ import print_function\r\n",
        "import sys\r\n",
        "\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import GRU\r\n",
        "from keras.layers import SimpleRNN\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import concatenate\r\n",
        "from keras import losses\r\n",
        "from keras import regularizers\r\n",
        "from keras.constraints import min_max_norm\r\n",
        "import h5py\r\n",
        "\r\n",
        "from keras.constraints import Constraint\r\n",
        "from keras import backend as K\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "#import tensorflow as tf\r\n",
        "#from keras.backend.tensorflow_backend import set_session\r\n",
        "#config = tf.ConfigProto()\r\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.42\r\n",
        "#set_session(tf.Session(config=config))\r\n",
        "\r\n",
        "\r\n",
        "def my_crossentropy(y_true, y_pred):\r\n",
        "    return K.mean(2*K.abs(y_true-0.5) * K.binary_crossentropy(y_pred, y_true), axis=-1)\r\n",
        "\r\n",
        "def mymask(y_true):\r\n",
        "    return K.minimum(y_true+1., 1.)\r\n",
        "\r\n",
        "def msse(y_true, y_pred):\r\n",
        "    return K.mean(mymask(y_true) * K.square(K.sqrt(y_pred) - K.sqrt(y_true)), axis=-1)\r\n",
        "\r\n",
        "def mycost(y_true, y_pred):\r\n",
        "    return K.mean(mymask(y_true) * (10*K.square(K.square(K.sqrt(y_pred) - K.sqrt(y_true))) + K.square(K.sqrt(y_pred) - K.sqrt(y_true)) + 0.01*K.binary_crossentropy(y_pred, y_true)), axis=-1)\r\n",
        "\r\n",
        "def my_accuracy(y_true, y_pred):\r\n",
        "    return K.mean(2*K.abs(y_true-0.5) * K.equal(y_true, K.round(y_pred)), axis=-1)\r\n",
        "\r\n",
        "class WeightClip(Constraint):\r\n",
        "    '''Clips the weights incident to each hidden unit to be inside a range\r\n",
        "    '''\r\n",
        "    def __init__(self, c=2):\r\n",
        "        self.c = c\r\n",
        "\r\n",
        "    def __call__(self, p):\r\n",
        "        return K.clip(p, -self.c, self.c)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        return {'name': self.__class__.__name__,\r\n",
        "            'c': self.c}\r\n",
        "\r\n",
        "reg = 0.000001\r\n",
        "constraint = WeightClip(0.499)\r\n",
        "\r\n",
        "print('Build model...')\r\n",
        "main_input = Input(shape=(None, 42), name='main_input')\r\n",
        "tmp = Dense(24, activation='tanh', name='input_dense', kernel_constraint=constraint, bias_constraint=constraint)(main_input)\r\n",
        "vad_gru = GRU(24, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name='vad_gru', kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg), kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(tmp)\r\n",
        "vad_output = Dense(1, activation='sigmoid', name='vad_output', kernel_constraint=constraint, bias_constraint=constraint)(vad_gru)\r\n",
        "noise_input = keras.layers.concatenate([tmp, vad_gru, main_input])\r\n",
        "noise_gru = GRU(48, activation='relu', recurrent_activation='sigmoid', return_sequences=True, name='noise_gru', kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg), kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(noise_input)\r\n",
        "denoise_input = keras.layers.concatenate([vad_gru, noise_gru, main_input])\r\n",
        "\r\n",
        "denoise_gru = GRU(96, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name='denoise_gru', kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg), kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(denoise_input)\r\n",
        "\r\n",
        "denoise_output = Dense(22, activation='sigmoid', name='denoise_output', kernel_constraint=constraint, bias_constraint=constraint)(denoise_gru)\r\n",
        "\r\n",
        "model = Model(inputs=main_input, outputs=[denoise_output, vad_output])\r\n",
        "\r\n",
        "model.compile(loss=[mycost, my_crossentropy],\r\n",
        "              metrics=[msse],\r\n",
        "              optimizer='adam', loss_weights=[10, 0.5])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H7d2X2kWVDx",
        "outputId": "ed386edf-1625-4b9a-9a1f-e3cf155317a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install audiosegment"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJCvDcZM7pWf",
        "outputId": "1e32601d-418a-4a5e-dbf4-5cb49d364fd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Example for plotting the FFT using this function\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "seg = audiosegment.from_file(list_breath_path[0])\r\n",
        "\r\n",
        "print(len(seg))\r\n",
        "# Just take the first 3 seconds\r\n",
        "hist_bins, hist_vals = seg[1:4000].fft()\r\n",
        "hist_vals_real_normed = np.abs(hist_vals) / len(hist_vals)\r\n",
        "plt.plot(hist_bins / 1000, hist_vals_real_normed)\r\n",
        "plt.xlabel(\"kHz\")\r\n",
        "plt.ylabel(\"dB\")\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "IJXAEoDT7klW",
        "outputId": "9799e910-e9cf-4d83-e49f-b6c3f0313a61"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install --upgrade quantecon"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa4Ohfis-Tpv",
        "outputId": "f0b32c09-1c00-4819-d70e-3ec73b35a474"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from scipy import linalg\r\n",
        "import numpy as np\r\n",
        "import matplotlib.cm as cm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "from quantecon import Kalman, LinearStateSpace\r\n",
        "from scipy.stats import norm\r\n",
        "from scipy.integrate import quad\r\n",
        "from numpy.random import multivariate_normal\r\n",
        "from scipy.linalg import eigvals\r\n",
        "\r\n",
        "ϵ = 0.1\r\n",
        "θ = 10  # Constant value of state x_t\r\n",
        "A, C, G, H = 1, 0, 1, 1\r\n",
        "ss = LinearStateSpace(A, C, G, H, mu_0=θ)\r\n",
        "\r\n",
        "x_hat_0, Σ_0 = 8, 1\r\n",
        "kalman = Kalman(ss, x_hat_0, Σ_0)\r\n",
        "\r\n",
        "T = len(seg)\r\n",
        "z = np.empty(T)\r\n",
        "x, y = seg[1:4000].fft()\r\n",
        "y = y.flatten()\r\n",
        "\r\n",
        "\r\n",
        "print(len(x))\r\n",
        "print(len(y))\r\n",
        "\r\n",
        "for t in range(T):\r\n",
        "    # Record the current predicted mean and variance and plot their densities\r\n",
        "    m, v = [temp for temp in (kalman.x_hat, kalman.Sigma)]\r\n",
        "\r\n",
        "    f = lambda x: norm.pdf(x, loc=m, scale=np.sqrt(v))\r\n",
        "    integral, error = quad(f, θ - ϵ, θ + ϵ)\r\n",
        "    z[t] = 1 - integral\r\n",
        "\r\n",
        "    kalman.update(y[t])\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\r\n",
        "ax.set_ylim(0, 1)\r\n",
        "ax.set_xlim(0, T)\r\n",
        "ax.plot(range(T), z)\r\n",
        "ax.fill_between(range(T), np.zeros(T), z, color=\"blue\", alpha=0.2)\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "e0hx3c9X8d3_",
        "outputId": "80e0470f-f2b5-4429-c5b5-3865b8576b3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Respiratory sound database"
      ],
      "metadata": {
        "id": "MPxR3Ycv6ja0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_GpmJ7OoED9",
        "outputId": "6d2a2597-dcf9-4899-ffe6-79ab957e1999"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install pydub"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjHKhApGpy6a",
        "outputId": "f68b5cc5-cac0-409d-de8b-74fb84a28c11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from glob import glob\n",
        "from shutil import copyfile\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "\n",
        "list_label_path = glob('/content/drive/My Drive/Breath-Data/public-data/label/*')\n",
        "list_breath_path = glob('/content/drive/My Drive/Breath-Data/public-data/*')"
      ],
      "outputs": [],
      "metadata": {
        "id": "Uq45W1TIoEUZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(len(list_label_path))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2wjwRYToEYI",
        "outputId": "086cc4ed-5d7b-4e53-f0f1-9051d62ac964"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from pydub import AudioSegment\n",
        "from pydub import AudioSegment, effects  \n",
        "# from rnnoise_wrapper import RNNoise\n",
        "\n",
        "# denoiser = RNNoise()\n",
        "\n",
        "# print(denoiser)\n",
        "\n",
        "duration_breath = 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "WM--hpt9oEb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_audio_segment (filename, source_path, destination_path, start, end, status, i):\n",
        "    \"\"\"[summary]\n",
        "    \n",
        "    Arguments:\n",
        "        filename {[type]} -- [filename of the audio file]\n",
        "        source_path {[type]} -- [source of a audio file]\n",
        "        destination_path {[type]} -- [destination of a output file]\n",
        "        start {[type]} -- [description]\n",
        "        end {[type]} -- [description]\n",
        "        status {[type]} -- [type of breath]\n",
        "    \"\"\"\n",
        "    \n",
        "    # Pydub works in milliseconds\n",
        "    start = start * 1000 \n",
        "    end = end * 1000\n",
        "    offset_time = (end - start)\n",
        "\n",
        "    global duration_breath\n",
        "    # print(offset_time)\n",
        "    print(duration_breath)\n",
        "\n",
        "    if offset_time > duration_breath and offset_time < 4000:\n",
        "      duration_breath = offset_time\n",
        "    \n",
        "    # print(start, end, end_2)\n",
        "    \n",
        "    # Get the audio file\n",
        "    src_Audio = AudioSegment.from_wav(source_path)\n",
        "    \n",
        "    # Get the audio offset\n",
        "    offset_Audio = AudioSegment.from_wav('/content/drive/My Drive/Breath-Data/Sine.wav')\n",
        "    \n",
        "    #Cut the right part\n",
        "    output_audio = src_Audio[start:end]\n",
        "\n",
        "    try:\n",
        "      if offset_time > 4000:\n",
        "        raise Exception(\"offset_time over 4000\")\n",
        "\n",
        "      # random start audio\n",
        "      rand = random.randint(0, int(4000 - offset_time))\n",
        "    \n",
        "    \n",
        "      #Convert to 5s\n",
        "      # output_audio = offset_Audio[0:rand] + output_audio[0:offset_time] + offset_Audio[rand+offset_time+1:4000]\n",
        "\n",
        "      ###########################################################################\n",
        "      for j in range(int(end - 4000), int(start), 100):\n",
        "        if j < 0:\n",
        "          continue\n",
        "        output_audio = src_Audio[j:j+4000]\n",
        "        # Define name file\n",
        "        fname =  filename + \"_\" + str(i) + \"_\" + str(j) + \"_\" + status + '.wav'\n",
        "        # print(destination_path)\n",
        "\n",
        "        # normalized sound\n",
        "        normalizedsound = effects.normalize(output_audio)\n",
        "        # denoised_audio = denoiser.filter(normalizedsound, sample_rate=8000, voice_prob_threshold=0.0, save_source_sample_rate=True)\n",
        "\n",
        "        # denoiser.write_wav(destination_path + '/' + fname, denoised_audio)\n",
        "        normalizedsound.export(destination_path + '/' + fname, \n",
        "                            format=\"wav\")  # Exports to a wav file in the current path.\n",
        "        # break\n",
        "      ###########################################################################\n",
        "      \n",
        "      # Define name file\n",
        "      # fname =  filename + \"_\" + str(i) + \"_\" + status + '.wav'\n",
        "      # print(destination_path)\n",
        "      # output_audio.export(destination_path + '/' + fname, \n",
        "      #                     format=\"wav\")  # Exports to a wav file in the current path.\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      pass"
      ],
      "outputs": [],
      "metadata": {
        "id": "z37f_UHAoEfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "time_normal = 0\r\n",
        "time_deep = 0\r\n",
        "count_normal = 0\r\n",
        "count_deep = 0\r\n",
        "\r\n",
        "count_crackle = 0\r\n",
        "count_wheeze = 0\r\n",
        "count_all_one = 0\r\n",
        "count_all_zero = 0\r\n",
        "\r\n",
        "\r\n",
        "def split_by_label(source_path, destination_path, label_path, output_folder):\r\n",
        "\r\n",
        "    # Check the output directory status \r\n",
        "\r\n",
        "    check_directory(destination_path, output_folder)\r\n",
        "\r\n",
        "    output_path = os.path.join(destination_path, output_folder) + '/'\r\n",
        "    # Get all the audio files\r\n",
        "    filenames = os.listdir(source_path)\r\n",
        "    \r\n",
        "    meta_data=[[\"\",\"\"]]\r\n",
        "    \r\n",
        "    # Go through all the file \r\n",
        "    for filename in filenames:\r\n",
        "        \r\n",
        "        # take the file name without dot\r\n",
        "        filename =  filename.split(\".\")[0]\r\n",
        "        \r\n",
        "        #get wav file name path\r\n",
        "        wav_path = source_path + filename + \".wav\"\r\n",
        "        \r\n",
        "        #get label filename path\r\n",
        "        csv_path = label_path + filename + \".txt\"\r\n",
        "        \r\n",
        "        #read the label file path \r\n",
        "#         label = pd.read_csv(csv_path, delim_whitespace= True)\r\n",
        "\r\n",
        "        if not os.path.isfile(csv_path):\r\n",
        "            print(\"Not found:\", filename)\r\n",
        "            continue\r\n",
        "\r\n",
        "        # Open the file with read only permit\r\n",
        "        label = open(csv_path, \"r\")\r\n",
        "        # use readlines to read all lines in the file\r\n",
        "        # The variable \"lines\" is a list containing all lines in the file\r\n",
        "        lines = label.readlines()\r\n",
        "        \r\n",
        "#         # split the text\r\n",
        "#         words = text.split()\r\n",
        "        \r\n",
        "        # close the file after reading the lines.\r\n",
        "        label.close()\r\n",
        "        \r\n",
        "        print(csv_path)\r\n",
        "\r\n",
        "        # break\r\n",
        "        \r\n",
        "        i = 0\r\n",
        "        breath = ''\r\n",
        "        \r\n",
        "        #Normal breath\r\n",
        "        for line in lines:\r\n",
        "            breath_start = float(line.split()[0])\r\n",
        "            breath_end   = float(line.split()[1])\r\n",
        "\r\n",
        "            # print(breath_start)\r\n",
        "            # print(breath_end)\r\n",
        "            breath_label_crackle = float(line.split()[2])\r\n",
        "            breath_label_wheeze = float(line.split()[3])\r\n",
        "            # print(int(breath_label_crackle))\r\n",
        "            # print(int(breath_label_wheeze))\r\n",
        "\r\n",
        "            if (int(breath_label_crackle) == 1 and int(breath_label_wheeze) == 1):\r\n",
        "              global count_all_one\r\n",
        "              count_all_one += 1\r\n",
        "\r\n",
        "              #######################\r\n",
        "              breath = 'crackle'\r\n",
        "              output_by_label = os.path.join(output_path, breath)               \r\n",
        "              if not os.path.exists(output_by_label):\r\n",
        "                  os.makedirs(output_by_label)\r\n",
        "              \r\n",
        "              # #Export the file\r\n",
        "              get_audio_segment(filename, wav_path, output_by_label, breath_start, breath_end, breath, i)\r\n",
        "              \r\n",
        "              i += 1\r\n",
        "\r\n",
        "              ################\r\n",
        "              breath = 'wheeze'\r\n",
        "              output_by_label = os.path.join(output_path, breath)               \r\n",
        "              if not os.path.exists(output_by_label):\r\n",
        "                  os.makedirs(output_by_label)\r\n",
        "              \r\n",
        "              # #Export the file\r\n",
        "              get_audio_segment(filename, wav_path, output_by_label, breath_start, breath_end, breath, i)\r\n",
        "              \r\n",
        "              i += 1\r\n",
        "\r\n",
        "              ##########\r\n",
        "              continue\r\n",
        "            elif (int(breath_label_crackle) == 1):\r\n",
        "              global count_crackle\r\n",
        "              count_crackle += 1\r\n",
        "              breath = 'crackle'\r\n",
        "            elif (int(breath_label_wheeze) == 1):\r\n",
        "              global count_wheeze\r\n",
        "              count_wheeze += 1\r\n",
        "              breath = 'wheeze'\r\n",
        "            else:\r\n",
        "              global count_all_zero\r\n",
        "              count_all_zero += 1\r\n",
        "              breath = 'normal'\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "            # break\r\n",
        "        # break\r\n",
        "            # try:\r\n",
        "            #     breath = line.split()[2]\r\n",
        "            #     if breath == 'strong':\r\n",
        "            #       breath = 'heavy'\r\n",
        "            #     if breath == 'normla':\r\n",
        "            #       breath = 'normal'\r\n",
        "            # except Exception as e:\r\n",
        "            #     print(e)\r\n",
        "            #     continue\r\n",
        "            \r\n",
        "            # time_breath = (breath_end - breath_start)\r\n",
        "            # if breath == 'normal' and time_breath > 1.73:\r\n",
        "            #   continue\r\n",
        "            #   # global time_normal\r\n",
        "            #   # time_normal += (breath_end - breath_start)\r\n",
        "            #   # global count_normal\r\n",
        "            #   # count_normal += 1\r\n",
        "\r\n",
        "            # if breath == 'deep' and time_breath < 2.52:\r\n",
        "            #   continue\r\n",
        "\r\n",
        "            #   # global time_deep\r\n",
        "            #   # time_deep += (breath_end - breath_start)\r\n",
        "            #   # global count_deep\r\n",
        "            #   # count_deep += 1\r\n",
        "\r\n",
        "            output_by_label = os.path.join(output_path, breath)               \r\n",
        "            if not os.path.exists(output_by_label):\r\n",
        "                os.makedirs(output_by_label)\r\n",
        "            \r\n",
        "            # # #Export the file\r\n",
        "            # get_audio_segment(filename, wav_path, output_by_label, breath_start, breath_end, breath, i)\r\n",
        "            \r\n",
        "            # i += 1\r\n",
        "\r\n",
        "            # # check pre and nextline label:\r\n",
        "            \r\n",
        "            # try:\r\n",
        "            #     nextline = lines[lines.index(line)+1]\r\n",
        "            #     breath_next = nextline.split()[2]\r\n",
        "            #     if breath_next == 'strong':\r\n",
        "            #       breath_next = 'heavy'\r\n",
        "            #     if breath_next == 'normla':\r\n",
        "            #       breath_next = 'normal'\r\n",
        "            # except Exception as e:\r\n",
        "            #     print(e)\r\n",
        "            #     continue\r\n",
        "\r\n",
        "            # try:\r\n",
        "            #     preline = lines[lines.index(line)+1]\r\n",
        "            #     breath_pre = nextline.split()[2]\r\n",
        "            #     if breath_pre == 'strong':\r\n",
        "            #       breath_pre = 'heavy'\r\n",
        "            #     if breath_pre == 'normla':\r\n",
        "            #       breath_pre = 'normal'\r\n",
        "            # except Exception as e:\r\n",
        "            #     print(e)\r\n",
        "            #     continue\r\n",
        "\r\n",
        "            # if breath != breath_pre or breath != breath_next:\r\n",
        "            #   continue\r\n",
        "            \r\n",
        "\r\n",
        "            # #Export the file\r\n",
        "            get_audio_segment(filename, wav_path, output_by_label, breath_start, breath_end, breath, i)\r\n",
        "            \r\n",
        "            i += 1\r\n",
        "            \r\n",
        "            # # Add other label\r\n",
        "            "
      ],
      "outputs": [],
      "metadata": {
        "id": "Z63cx1FL1QiT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Dev set\r\n",
        "DEV_INPUT_PATH = \"/content/drive/My Drive/Breath-Data/public-data/\"\r\n",
        "DEV_OUTPUT_PATH = \"/content/data/public/\"\r\n",
        "DEV_LABEL_PATH = \"/content/drive/My Drive/Breath-Data/public-data/label/\"\r\n",
        "\r\n",
        "TRAIN_OUTPUT_FOLDER = \"training\"\r\n",
        "TEST_OUTPUT_FOLDER = \"validation\"\r\n",
        "\r\n",
        "\r\n",
        "# Test set\r\n",
        "TEST_INPUT_PATH = \"/content/gdrive/My Drive/Breath-Data/raw_data/validation/\"\r\n",
        "TEST_OUTPUT_PATH = \"/content/data/public/\"\r\n",
        "TEST_LABEL_PATH = \"/content/gdrive/My Drive/Breath-Data/raw_data/validation/label/\"\r\n",
        "\r\n",
        "CHUNK = 5\r\n",
        "OVERLAP = 0.5\r\n",
        "\r\n",
        "LABEL_FOLDER = 'label'\r\n",
        " # Type of breath\r\n",
        "BREATH_TYPE = ['normal', 'deep', 'heavy', 'other']"
      ],
      "outputs": [],
      "metadata": {
        "id": "P69MT5Gk1QmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def check_directory(origin_path, folder):\r\n",
        "    directory = os.path.join(origin_path, folder)\r\n",
        "    if not os.path.exists(directory):\r\n",
        "        os.makedirs(directory)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7AFaXoQ91QqY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Training set\r\n",
        "split_by_label(DEV_INPUT_PATH, DEV_OUTPUT_PATH, DEV_LABEL_PATH, TRAIN_OUTPUT_FOLDER)\r\n",
        "\r\n",
        "print(count_crackle)\r\n",
        "print(count_wheeze)\r\n",
        "print(count_all_one)\r\n",
        "print(count_all_zero)\r\n",
        "\r\n",
        "# Validation set\r\n",
        "# split_by_label(TEST_INPUT_PATH, TEST_OUTPUT_PATH, TEST_LABEL_PATH, TEST_OUTPUT_FOLDER)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMRNWJwf1jqi",
        "outputId": "e03baaa2-f08b-40e2-88f9-55d24cb50521"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list_file = glob('/content/drive/My Drive/Breath-Data/public-data/label/*')"
      ],
      "outputs": [],
      "metadata": {
        "id": "7uC10acU1Quy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(len(list_file))\r\n",
        "list_name = []\r\n",
        "for name in list_file:\r\n",
        "  # print(name.split('/')[-1].split('_')[0])\r\n",
        "  if name.split('/')[-1].split('_')[0] not in list_name:\r\n",
        "    list_name.append(name.split('/')[-1].split('_')[0])\r\n",
        "  # break"
      ],
      "outputs": [],
      "metadata": {
        "id": "7Yr0WlgS1Qy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3109e7-0099-4f1d-da42-a077b6cabc8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(len(list_name))"
      ],
      "outputs": [],
      "metadata": {
        "id": "_h6R2D1MoEjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7a56cc-6665-424a-c5a8-de874a60fe9b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLMvvNccxsGG",
        "outputId": "856d8941-5711-4b5c-f782-06a0e7da3d99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from glob import glob\r\n",
        "from shutil import copyfile\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from pydub import AudioSegment\r\n",
        "\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/public-data/*')\r\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Breath-Data/patient_diagnosis.csv')\r\n",
        "\r\n",
        "print(\"df - number = \", df.number[0])\r\n",
        "print(\"df - label = \", df.label[0])\r\n",
        "print(\"df - len = \", len(df))\r\n",
        "\r\n",
        "print('len list = ', len(list_breath_path))\r\n",
        "\r\n",
        "for path in list_breath_path:\r\n",
        "  print(path)\r\n",
        "  print('file name = ', path.split('/')[-1])\r\n",
        "  # print('number file name = ', path.split('/')[-1].split('_')[0])\r\n",
        "  label = None\r\n",
        "\r\n",
        "  for i in range(0, len(df)):\r\n",
        "    # print(df.number[i])\r\n",
        "    if int(df.number[i]) == int(path.split('/')[-1].split('_')[0]):\r\n",
        "      print('label = ', df.label[i])\r\n",
        "      label = df.label[i]\r\n",
        "      break\r\n",
        "\r\n",
        "  # output_dist = '/content/gdrive/My Drive/Breath-Data/data-public/' + label\r\n",
        "  output_dist = '/content/data-public/' + label\r\n",
        "  if not os.path.exists(output_dist):\r\n",
        "    os.makedirs(output_dist)\r\n",
        "  \r\n",
        "\r\n",
        "  audio_file = AudioSegment.from_wav(path)\r\n",
        "\r\n",
        "  if label == 'COPD':\r\n",
        "    for i in range(1, int(len(audio_file)/4000)):\r\n",
        "      # print(len(audio_file[4000*(i-1):4000*i]))\r\n",
        "      output_dist_1 = output_dist + '/' + str(i) + '_' +  path.split('/')[-1]\r\n",
        "      audio_file[4000*(i-1):4000*i].export(output_dist_1, format=\"wav\")\r\n",
        "    #   break\r\n",
        "\r\n",
        "    # break\r\n",
        "  else:\r\n",
        "    for i in range(1, int(len(audio_file)/100)):\r\n",
        "      # print(len(audio_file[100*(i-1):100*(i-1)+4000]))\r\n",
        "      output_dist_1 = output_dist + '/' + str(i) + '_' +  path.split('/')[-1]\r\n",
        "      audio_file[100*(i-1):100*(i-1)+4000].export(output_dist_1, format=\"wav\")\r\n",
        "    #   break\r\n",
        "    # break\r\n",
        "  # break"
      ],
      "outputs": [],
      "metadata": {
        "id": "MDtYNyKs6j6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "b8dc26e7-7ee0-4e1f-9776-c9c7da5dc1e6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Bronchiolitis/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Bronchiectasis/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/LRTI/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Asthma/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/URTI/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Pneumonia/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Healthy/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/COPD/*')\r\n",
        "print(len(list_breath_path))\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "g8Qt2w7H6yI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2e5208-170a-48ad-a56d-41eff661949f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Bronchiolitis/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Bronchiectasis/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/LRTI/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Asthma/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/URTI/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Pneumonia/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/Healthy/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/COPD/*')\r\n",
        "print(len(list_breath_path))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joyE9iPi8tbo",
        "outputId": "6e4a2eff-6bbb-463e-e32e-d99d633da560"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list_breath_path = glob('/content/data/public/training/crackle/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/data/public/training/normal/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/data/public/training/wheeze/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/data/public/training/abnormal/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "\r\n",
        "# 1304 - crackle\r\n",
        "# 2491 - normal\r\n",
        "# 605 - wheeze"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7w5S6WgmT1C",
        "outputId": "3e563a69-49d0-4a1c-8c44-c19cf0815bc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Copy file"
      ],
      "metadata": {
        "id": "pwBuT1E9zili"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(len(list_breath_path))\r\n",
        "print(int(len(list_breath_path)*20/100))\r\n",
        "print(list_breath_path[0].split('/')[-2])"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut-CFaZlznaw",
        "outputId": "3b2d2bff-2a6c-45b3-e4e2-4ffd8def3dcf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import shutil\r\n",
        "\r\n",
        "# Bronchiolitis\r\n",
        "# Bronchiectasis\r\n",
        "# LRTI\r\n",
        "# Asthma\r\n",
        "# URTI\r\n",
        "# Pneumonia\r\n",
        "# Healthy\r\n",
        "# COPD\r\n",
        "\r\n",
        "# list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/COPD/*')\r\n",
        "list_breath_path = glob('/content/data/public/training/normal/*')\r\n",
        "\r\n",
        "count = 0\r\n",
        "for path in list_breath_path:\r\n",
        "  count += 1\r\n",
        "  if count == int(len(list_breath_path)*20/100):\r\n",
        "    break\r\n",
        "  \r\n",
        "  output_dist = '/content/data-public/data-validation/' + path.split('/')[-2] + '/'\r\n",
        "  if not os.path.exists(output_dist):\r\n",
        "    os.makedirs(output_dist)\r\n",
        "  output_dist = output_dist + path.split('/')[-1]\r\n",
        "\r\n",
        "  shutil.move(path, output_dist)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Rd8x5D1eqJzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list_breath_path = glob('/content/data-public/data-validation/crackle/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/data-public/data-validation/normal/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/data-public/data-validation/wheeze/*')\r\n",
        "print(len(list_breath_path))\r\n",
        "list_breath_path = glob('/content/data-public/data-validation/abnormal/*')\r\n",
        "print(len(list_breath_path))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDs0apXrqJ2E",
        "outputId": "26103fff-f44a-4b3c-f313-3d181c0310ba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import shutil\r\n",
        "\r\n",
        "# Bronchiolitis\r\n",
        "# Bronchiectasis\r\n",
        "# LRTI\r\n",
        "# Asthma\r\n",
        "# URTI\r\n",
        "# Pneumonia\r\n",
        "# Healthy\r\n",
        "# COPD\r\n",
        "\r\n",
        "# list_breath_path = glob('/content/gdrive/My Drive/Breath-Data/data-public/COPD/*')\r\n",
        "list_breath_path = glob('/content/data/public/training/crackle/*')\r\n",
        "\r\n",
        "count = 0\r\n",
        "for path in list_breath_path:\r\n",
        "  count += 1\r\n",
        "  if count == int(len(list_breath_path)*100/100):\r\n",
        "    break\r\n",
        "  \r\n",
        "  output_dist = '/content/data/public/training/abnormal/'\r\n",
        "  if not os.path.exists(output_dist):\r\n",
        "    os.makedirs(output_dist)\r\n",
        "  output_dist = output_dist + path.split('/')[-1]\r\n",
        "\r\n",
        "  shutil.copy(path, output_dist)"
      ],
      "outputs": [],
      "metadata": {
        "id": "XFt7G8vdqJ5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from glob import glob\r\n",
        "heavy_path = glob('/content/gdrive/My Drive/Breath-Data/data/validation/heavy/*')\r\n",
        "print(len(heavy_path))\r\n",
        "deep_path = glob('/content/gdrive/My Drive/Breath-Data/data/training/deep/*')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvdyoa0jhOKn",
        "outputId": "4059eddf-b3f3-4e80-c323-e1bb05f4d67e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list_user = []\r\n",
        "count  = 0\r\n",
        "dictCount = {}\r\n",
        "for path in heavy_path:\r\n",
        "  user = path.split('/')[-1].split('_')[0]\r\n",
        "  # print(user)\r\n",
        "  if user not in list_user:\r\n",
        "    list_user.append(user)\r\n",
        "    dictCount.setdefault(user, 0)\r\n",
        "  \r\n",
        "  count += 1\r\n",
        "  # print(list_user)\r\n",
        "  # if count == 50:\r\n",
        "  #   break"
      ],
      "outputs": [],
      "metadata": {
        "id": "stY6FdIHiInq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(len(list_user))\r\n",
        "print(list_user)\r\n",
        "print(dictCount['01'])"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG7ic0BRjXzd",
        "outputId": "414f9f8a-0edb-49ba-8818-8e2f40df4462"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from glob import glob\r\n",
        "dst_path = '/content/gdrive/My Drive/Breath-Data/data/validation/breath/'\r\n",
        "src_path = glob('/content/gdrive/My Drive/Breath-Data/data/validation/deep/*')\r\n",
        "from shutil import copyfile\r\n",
        "count = 0\r\n",
        "\r\n",
        "\r\n",
        "for path in src_path:\r\n",
        "  user = path.split('/')[-1].split('_')[0]\r\n",
        "  file_name = path.split('/')[-1]\r\n",
        "  dst = dst_path + file_name\r\n",
        "  \r\n",
        "  # if count <= 112:\r\n",
        "  #   count += 1\r\n",
        "  if dictCount[user] <= 38:\r\n",
        "    dictCount[user] += 1\r\n",
        "    copyfile(path, dst)\r\n",
        "  # else:\r\n",
        "  #   break\r\n",
        "\r\n",
        "\r\n",
        "  # count += 1\r\n",
        "  # break"
      ],
      "outputs": [],
      "metadata": {
        "id": "UWaKAYw-ljYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from glob import glob\r\n",
        "breath_path_new = glob('/content/gdrive/My Drive/Breath-Data/data/validation/breath/*')\r\n",
        "\r\n",
        "print(len(breath_path_new))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFiguO86qxGI",
        "outputId": "695720bc-9b76-4008-ab8c-e5b0e1ad338d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Lib"
      ],
      "metadata": {
        "id": "aCG7D-HvdPkf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n",
        "import json\r\n",
        "import pickle\r\n",
        "from scipy.io import wavfile\r\n",
        "\r\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"0\"\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "with tf.device('/device:GPU:0'):\r\n",
        "\r\n",
        "    import tensorflow.keras as keras\r\n",
        "    import tensorflow as tf\r\n",
        "\r\n",
        "# Allow memory growth for the GPU\r\n",
        "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n",
        "# tf.config.experimental.set_memory_growth(physical_devices, True)\r\n",
        "\r\n",
        "# from tensorflow.keras.utils import multi_gpu_model\r\n",
        "# from keras.backend.tensorflow_backend import set_session\r\n",
        "    import librosa\r\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\r\n",
        "    from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
        "    from tensorflow.keras.utils import to_categorical\r\n",
        "    from tensorflow.keras.models import Sequential\r\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, MaxPooling1D, Conv1D\r\n",
        "    from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Permute, Reshape, TimeDistributed\r\n",
        "    from tensorflow.keras.optimizers import Adam\r\n",
        "    from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Flatten\r\n",
        "    from tensorflow.compat.v1.keras.layers import CuDNNLSTM as CuLSTM\r\n",
        "    from tensorflow.compat.v1.keras.layers import Input, Dense, Lambda, Layer\r\n",
        "    from tensorflow.keras.layers import add\r\n",
        "    from tensorflow.keras.layers import Input\r\n",
        "    from tensorflow.keras.models import Model\r\n",
        "    from tensorflow.keras.layers import BatchNormalization\r\n",
        "    from tensorflow.keras.layers.experimental import preprocessing\r\n",
        "    from tensorflow.keras import regularizers\r\n",
        "    from tensorflow.keras.layers import LeakyReLU\r\n",
        "\r\n",
        "    normalize = preprocessing.Normalization()"
      ],
      "outputs": [],
      "metadata": {
        "id": "ydkuk4K6vEL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init Data Generator"
      ],
      "metadata": {
        "id": "M6u4kK2bc6uf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install pydub"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6CPaRU_iRCV",
        "outputId": "20d3ab3a-6527-4963-85bf-af812075ad44"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf\r\n",
        "import h5py\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from tensorflow.python.keras import backend as K\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.layers import MaxPooling2D, Conv2D, Dense, Activation, Flatten, Dropout, Input\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop"
      ],
      "outputs": [],
      "metadata": {
        "id": "v4UCWk7OiXNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import sklearn\r\n",
        "from librosa.util import normalize\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "from pydub import AudioSegment, effects \r\n",
        "def normalize_manual(x, axis=0):\r\n",
        "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\r\n",
        "\r\n",
        "def normalize_fixed(x, current_range =[[0, 100]], normed_range=[[0, 1]]):\r\n",
        "    current_min, current_max = tf.expand_dims(current_range[:, 0], 1), tf.expand_dims(current_range[:, 1], 1)\r\n",
        "    normed_min, normed_max = tf.expand_dims(normed_range[:, 0], 1), tf.expand_dims(normed_range[:, 1], 1)\r\n",
        "    x_normed = (x - current_min) / (current_max - current_min)\r\n",
        "    x_normed = x_normed * (normed_max - normed_min) + normed_min\r\n",
        "    return x_normed\r\n",
        "\r\n",
        "def despike(yi, th=1.e-8):\r\n",
        "    '''Remove spike from array yi, the spike area is where the difference between the neigboring points is higher than th.'''\r\n",
        "    y = np.copy(yi) # use y = y1 if it is OK to modify input array\r\n",
        "    n = len(y)\r\n",
        "    x = np.arange(n)\r\n",
        "    c = np.argmax(y)\r\n",
        "    d = abs(np.diff(y))\r\n",
        "    try:\r\n",
        "        l = c - 1 - np.where(d[c-1::-1]<th)[0][0]\r\n",
        "        r = c + np.where(d[c:]<th)[0][0] + 1\r\n",
        "    except: # no spike, return unaltered array\r\n",
        "        return y\r\n",
        "    # for fit, use area twice wider then the spike\r\n",
        "    if (r-l) <= 3:\r\n",
        "        l -= 1\r\n",
        "        r += 1\r\n",
        "    s = int(round((r-l)/2.))\r\n",
        "    lx = l - s\r\n",
        "    rx = r + s\r\n",
        "    # make a gap at spike area\r\n",
        "    xgapped = np.concatenate((x[lx:l],x[r:rx]))\r\n",
        "    ygapped = np.concatenate((y[lx:l],y[r:rx]))\r\n",
        "    # quadratic fit of the gapped array\r\n",
        "    z = np.polyfit(xgapped,ygapped,2)\r\n",
        "    p = np.poly1d(z)\r\n",
        "    y[l:r] = p(x[l:r])\r\n",
        "    return y\r\n",
        "\r\n",
        "class BreathDataTrainingGenerator(tf.keras.utils.Sequence):#tensorflow.python.keras.utils.data_utils --- tf.keras.utils.Sequence\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self, directory, \r\n",
        "                    list_labels=['normal', 'deep', 'heavy', 'other'], \r\n",
        "                    batch_size=32,\r\n",
        "                    dim=None,\r\n",
        "                    classes=None, \r\n",
        "                    shuffle=True):\r\n",
        "        'Initialization'\r\n",
        "        self.directory = directory\r\n",
        "        self.list_labels = list_labels\r\n",
        "        self.dim = dim\r\n",
        "        self.__flow_from_directory(self.directory)\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.classes = len(self.list_labels)\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.on_epoch_end()\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        'Denotes the number of batches per epoch'\r\n",
        "        return int(np.floor(len(self.wavs) / self.batch_size))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        # print(\"In get Item!!\")\r\n",
        "        # 'Generate one batch of data'\r\n",
        "        # Generate indexes of the batch\r\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n",
        "\r\n",
        "        # Find list of IDs\r\n",
        "        rawX = [self.wavs[k] for k in indexes]\r\n",
        "        rawY = [self.labels[k] for k in indexes]\r\n",
        "\r\n",
        "        # Generate data\r\n",
        "        X, Y = self.__feature_extraction(rawX, rawY)\r\n",
        "        # print(\"TADDDDDDDD\", len([X, Y, Y]), \" - \", len([X, Y, Y][0]), \" - \", len([X, Y, Y][0][0]))\r\n",
        "        # print(\"Done getting data\")\r\n",
        "        return X, Y\r\n",
        "\r\n",
        "    def __flow_from_directory(self, directory):\r\n",
        "        self.wavs = []\r\n",
        "        self.labels = []\r\n",
        "        for dir in os.listdir(directory):\r\n",
        "            sub_dir = os.path.join(directory, dir)\r\n",
        "            if os.path.isdir(sub_dir) and dir in self.list_labels:\r\n",
        "                print(sub_dir)\r\n",
        "                label = self.list_labels.index(dir)\r\n",
        "                try:\r\n",
        "                    for file in os.listdir(sub_dir):\r\n",
        "                        if file == '22_male_21_VHung_41_225121_deep.wav':\r\n",
        "                            continue\r\n",
        "                        \r\n",
        "                        self.wavs.append(os.path.join(sub_dir, file))\r\n",
        "                        self.labels.append(label)\r\n",
        "                except Exception as e:\r\n",
        "                    print(e)\r\n",
        "                    print(file)\r\n",
        "                    pass\r\n",
        "\r\n",
        "\r\n",
        "    def on_epoch_end(self):\r\n",
        "        'Updates indexes after each epoch'\r\n",
        "        self.indexes = np.arange(len(self.wavs))\r\n",
        "        if self.shuffle == True:\r\n",
        "            np.random.shuffle(self.indexes)\r\n",
        "\r\n",
        "    def __feature_extraction(self, list_wav, list_label):\r\n",
        "        # print(\"Go to feature extraction!!!\")\r\n",
        "        'Generates data containing batch_size samples'\r\n",
        "        X = []\r\n",
        "        Y = []\r\n",
        "        for i in range(self.batch_size):\r\n",
        "            rate, data = wavfile.read(list_wav[i]) #bug in here\r\n",
        "\r\n",
        "            # data, rate = readCoughData(file=list_wav[i])\r\n",
        "\r\n",
        "\r\n",
        "            # resampledData = resample(originalData=data, origSampFreq=rate, targetSampFreq=16000)\r\n",
        "            # normalizedData = normalizeSound(resampledData, axis=0)\r\n",
        "            # S = calculateMelSpectogram(normalizedData=normalizedData, hop_length=512, win_length=1024, sr=16000)\r\n",
        "            # plotSound(soundData=normalizedData, sr=8000,x_axis_string='time')\r\n",
        "            # plotMelSpectogram(S, sr=8000, ref=np.max)\r\n",
        "\r\n",
        "\r\n",
        "            # data = effects.normalize(data)\r\n",
        "            # print(\"End\")\r\n",
        "            # data = despike(data)\r\n",
        "\r\n",
        "            data = np.array(data, dtype=np.float32)\r\n",
        "            data *= 1./32768\r\n",
        "            # feature = librosa.feature.melspectrogram(y=data, sr=rate, n_fft=2048, hop_length=512, power=2.0)\r\n",
        "            # feature = librosa.feature.rmse(data+ 0.0001)\r\n",
        "            # feature = librosa.feature.spectral_rolloff(y=data, sr=rate)[0]\r\n",
        "            feature = librosa.feature.mfcc(y=data, sr=rate, \r\n",
        "                                           n_mfcc=40, fmin=0, fmax=8000,\r\n",
        "                                           n_fft=int(16*64), hop_length=int(16*32), power=2.0)\r\n",
        "\r\n",
        "            # print(S.shape) # (128, 251)\r\n",
        "            # feature = normalize(feature)\r\n",
        "            feature = np.resize(feature, self.dim)\r\n",
        "\r\n",
        "            # mellog = np.log(feature + 1e-9)\r\n",
        "            # feature = librosa.util.normalize(mellog)\r\n",
        "            # feature= sklearn.preprocessing.normalize(feature)\r\n",
        "\r\n",
        "            # normalize data\r\n",
        "            # feature = normalize(feature)\r\n",
        "\r\n",
        "            category_label =  to_categorical(list_label[i], num_classes= len(self.list_labels) )\r\n",
        "            X.append(feature)\r\n",
        "            Y.append(category_label)\r\n",
        "        \r\n",
        "        # print(X)\r\n",
        "        # X = normalize_fixed(X)\r\n",
        "        # X -= np.mean(X, keepdims=True)\r\n",
        "        # X = (X- np.min(X, 0)) / (np.max(X, 0) + 0.0001) \r\n",
        "        # X /= (np.std(X, keepdims=True) + tf.keras.backend.epsilon())\r\n",
        "        X = np.array(X, dtype=np.float32)\r\n",
        "        mean = np.mean(X, axis=0)\r\n",
        "        std = np.std(X, axis=0)\r\n",
        "\r\n",
        "        X = (X - mean)/std\r\n",
        "\r\n",
        "        # print(len(X[0]))\r\n",
        "        # print(len(X[0][0]))\r\n",
        "\r\n",
        "        # min_max_scaler = preprocessing.MinMaxScaler()\r\n",
        "        \r\n",
        "        Y = np.array(Y, dtype=int)\r\n",
        "        return X, Y\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "ptOPnPWdc9ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Generator Multitask"
      ],
      "metadata": {
        "id": "dZdhyUAvN2rv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import sklearn\r\n",
        "from librosa.util import normalize\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "from pydub import AudioSegment, effects \r\n",
        "\r\n",
        "class BreathDataTrainingGeneratorMulti(tf.keras.utils.Sequence):#tensorflow.python.keras.utils.data_utils --- tf.keras.utils.Sequence\r\n",
        "    'Generates data for Keras'\r\n",
        "    def __init__(self, directory, \r\n",
        "                    list_labels=['normal', 'deep', 'heavy', 'other'], \r\n",
        "                    batch_size=32,\r\n",
        "                    dim=None,\r\n",
        "                    classes=None, \r\n",
        "                    shuffle=True):\r\n",
        "        'Initialization'\r\n",
        "        self.directory = directory\r\n",
        "        self.list_labels = list_labels\r\n",
        "        self.dim = dim\r\n",
        "        self.__flow_from_directory(self.directory)\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.classes = len(self.list_labels)\r\n",
        "        self.shuffle = shuffle\r\n",
        "        self.on_epoch_end()\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        'Denotes the number of batches per epoch'\r\n",
        "        return int(np.floor(len(self.wavs) / self.batch_size))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        # print(\"In get Item!!\")\r\n",
        "        # 'Generate one batch of data'\r\n",
        "        # Generate indexes of the batch\r\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n",
        "\r\n",
        "        # Find list of IDs\r\n",
        "        rawX = [self.wavs[k] for k in indexes]\r\n",
        "        rawY = [self.labels[k] for k in indexes]\r\n",
        "\r\n",
        "        # Generate data\r\n",
        "        X, Y, Y1 = self.__feature_extraction(rawX, rawY)\r\n",
        "        # print(\"TADDDDDDDD\", len([X, Y, Y]), \" - \", len([X, Y, Y][0]), \" - \", len([X, Y, Y][0][0]))\r\n",
        "        # print(\"Done getting data\")\r\n",
        "        return X, [Y, Y1]\r\n",
        "\r\n",
        "    def __flow_from_directory(self, directory):\r\n",
        "        self.wavs = []\r\n",
        "        self.labels = []\r\n",
        "        for dir in os.listdir(directory):\r\n",
        "            sub_dir = os.path.join(directory, dir)\r\n",
        "            if os.path.isdir(sub_dir) and dir in self.list_labels:\r\n",
        "                print(sub_dir)\r\n",
        "                label = self.list_labels.index(dir)\r\n",
        "                try:\r\n",
        "                    for file in os.listdir(sub_dir):\r\n",
        "                        if file == '22_male_21_VHung_41_225121_deep.wav':\r\n",
        "                            continue\r\n",
        "                        \r\n",
        "                        self.wavs.append(os.path.join(sub_dir, file))\r\n",
        "                        self.labels.append(label)\r\n",
        "                except Exception as e:\r\n",
        "                    print(e)\r\n",
        "                    print(file)\r\n",
        "                    pass\r\n",
        "\r\n",
        "\r\n",
        "    def on_epoch_end(self):\r\n",
        "        'Updates indexes after each epoch'\r\n",
        "        self.indexes = np.arange(len(self.wavs))\r\n",
        "        if self.shuffle == True:\r\n",
        "            np.random.shuffle(self.indexes)\r\n",
        "\r\n",
        "    def __feature_extraction(self, list_wav, list_label):\r\n",
        "        # print(\"Go to feature extraction!!!\")\r\n",
        "        'Generates data containing batch_size samples'\r\n",
        "        X = []\r\n",
        "        Y = []\r\n",
        "        Y1 = []\r\n",
        "        for i in range(self.batch_size):\r\n",
        "            rate, data = wavfile.read(list_wav[i]) #bug in here\r\n",
        "\r\n",
        "            # data, rate = readCoughData(file=list_wav[i])\r\n",
        "\r\n",
        "\r\n",
        "            # resampledData = resample(originalData=data, origSampFreq=rate, targetSampFreq=16000)\r\n",
        "            # normalizedData = normalizeSound(resampledData, axis=0)\r\n",
        "            # S = calculateMelSpectogram(normalizedData=normalizedData, hop_length=512, win_length=1024, sr=16000)\r\n",
        "            # plotSound(soundData=normalizedData, sr=8000,x_axis_string='time')\r\n",
        "            # plotMelSpectogram(S, sr=8000, ref=np.max)\r\n",
        "\r\n",
        "\r\n",
        "            # data = effects.normalize(data)\r\n",
        "            # print(\"End\")\r\n",
        "            # data = despike(data)\r\n",
        "\r\n",
        "            data = np.array(data, dtype=np.float32)\r\n",
        "            data *= 1./32768\r\n",
        "            # feature = librosa.feature.melspectrogram(y=data, sr=rate, n_fft=2048, hop_length=512, power=2.0)\r\n",
        "            # feature = librosa.feature.rmse(data+ 0.0001)\r\n",
        "            # feature = librosa.feature.spectral_rolloff(y=data, sr=rate)[0]\r\n",
        "            feature = librosa.feature.mfcc(y=data, sr=rate, \r\n",
        "                                           n_mfcc=40, fmin=0, fmax=8000,\r\n",
        "                                           n_fft=int(16*64), hop_length=int(16*32), power=2.0)\r\n",
        "\r\n",
        "            # print(S.shape) # (128, 251)\r\n",
        "            # feature = normalize(feature)\r\n",
        "            feature = np.resize(feature, self.dim)\r\n",
        "\r\n",
        "            # mellog = np.log(feature + 1e-9)\r\n",
        "            # feature = librosa.util.normalize(mellog)\r\n",
        "            # feature= sklearn.preprocessing.normalize(feature)\r\n",
        "\r\n",
        "            # normalize data\r\n",
        "            # feature = normalize(feature)\r\n",
        "\r\n",
        "            category_label =  to_categorical(list_label[i], num_classes= len(self.list_labels) )\r\n",
        "            if list_label[i] == 1:\r\n",
        "                category_label1 =  to_categorical(0, num_classes= 2) # non-breath\r\n",
        "            else:\r\n",
        "                category_label1 =  to_categorical(1, num_classes= 2) # breath\r\n",
        "            X.append(feature)\r\n",
        "            Y.append(category_label)\r\n",
        "            Y1.append(category_label1)\r\n",
        "        \r\n",
        "        # print(X)\r\n",
        "        # X = normalize_fixed(X)\r\n",
        "        # X -= np.mean(X, keepdims=True)\r\n",
        "        # X = (X- np.min(X, 0)) / (np.max(X, 0) + 0.0001) \r\n",
        "        # X /= (np.std(X, keepdims=True) + tf.keras.backend.epsilon())\r\n",
        "        X = np.array(X, dtype=np.float32)\r\n",
        "        mean = np.mean(X, axis=0)\r\n",
        "        std = np.std(X, axis=0)\r\n",
        "\r\n",
        "        X = (X - mean)/std\r\n",
        "\r\n",
        "        # print(len(X[0]))\r\n",
        "        # print(len(X[0][0]))\r\n",
        "\r\n",
        "        # min_max_scaler = preprocessing.MinMaxScaler()\r\n",
        "        \r\n",
        "        Y = np.array(Y, dtype=int)\r\n",
        "        Y1 = np.array(Y1, dtype=int)\r\n",
        "        return X, Y, Y1\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "uRV7Ra-BIxey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contant"
      ],
      "metadata": {
        "id": "v1UBdKA-kbsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classify"
      ],
      "metadata": {
        "id": "3Sgld0GrwH01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s4ntm7ax5yEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Set the config for training\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "# LIST_LABELS = ['normal', 'deep', 'heavy', 'other']\r\n",
        "LIST_LABELS = ['normal', 'deep', 'heavy', 'other']\r\n",
        "N_CLASSES = len(LIST_LABELS)\r\n",
        "N_EPOCHS = 100\r\n",
        "\r\n",
        "# INPUT_SIZE = (40, 140, 1) # Input size for CNN training\r\n",
        "# INPUT_SIZE = (40, 128) # Input size for LSTM training\r\n",
        "# INPUT_SIZE = (32, 128, 1) # Input size for CNN training\r\n",
        "INPUT_SIZE = (32, 128) # Input size for LSTM training\r\n",
        "\r\n",
        "\r\n",
        "TRAINING_SOURCE = '/content/data/public/training/'\r\n",
        "VALID_SOURCE = '/content/data-public/data-validation/'\r\n",
        "MODE = 'TRAINING'\r\n",
        "MODEL_OUTPUT = '/content/drive/My Drive/Breath-Data/data/model_output'\r\n",
        "\r\n",
        "RUN_TITLE = \"20210805-\""
      ],
      "outputs": [],
      "metadata": {
        "id": "6mc3Km_rkagi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# !cp -r '/content/gdrive/My Drive/Breath-Data/data-public/' '/content/'\r\n",
        "\r\n",
        "# Set the config for training\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "# LIST_LABELS = ['normal', 'deep', 'heavy', 'other']\r\n",
        "# LIST_LABELS = ['Bronchiolitis', 'Bronchiectasis', 'LRTI', 'Asthma', 'URTI', 'Pneumonia', 'Healthy', 'COPD']\r\n",
        "# ['Bronchiolitis', 'Bronchiectasis', 'LRTI', 'Asthma', 'URTI', 'Pneumonia', 'Healthy', 'COPD']\r\n",
        "LIST_LABELS = ['crackle', 'normal', 'wheeze']\r\n",
        "# LIST_LABELS = ['abnormal', 'normal']\r\n",
        "N_CLASSES = len(LIST_LABELS)\r\n",
        "N_EPOCHS = 20\r\n",
        "\r\n",
        "# INPUT_SIZE = (40, 126, 1) # Input size for CNN training\r\n",
        "# INPUT_SIZE = (40, 126) # Input size for LSTM training\r\n",
        "# INPUT_SIZE = (32, 140, 1) # Input size for CNN training\r\n",
        "INPUT_SIZE = (32, 251) # Input size for LSTM training\r\n",
        "\r\n",
        "TRAINING_SOURCE = '/content/data/public/training/'\r\n",
        "VALID_SOURCE = '/content/data-public/data-validation/'\r\n",
        "# TRAINING_SOURCE = '/content/gdrive/My Drive/Breath-Data/data-public/'\r\n",
        "# VALID_SOURCE = '/content/gdrive/My Drive/Breath-Data/data-public/data-validation/'\r\n",
        "MODE = 'TRAINING'\r\n",
        "MODEL_OUTPUT = '/content/gdrive/My Drive/Breath-Data/data/model_output'\r\n",
        "\r\n",
        "RUN_TITLE = \"data-public\""
      ],
      "outputs": [],
      "metadata": {
        "id": "TOx7DQbc81ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detection"
      ],
      "metadata": {
        "id": "ujt7lRG5wOXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Set the config for training\r\n",
        "\r\n",
        "BATCH_SIZE = 64\r\n",
        "# LIST_LABELS = ['normal', 'deep', 'heavy', 'other']\r\n",
        "LIST_LABELS = ['normal', 'abnormal']\r\n",
        "N_CLASSES = len(LIST_LABELS)\r\n",
        "N_EPOCHS = 10\r\n",
        "\r\n",
        "\r\n",
        "# INPUT_SIZE = (32, 140, 1) # Input size for CNN training\r\n",
        "INPUT_SIZE = (32, 251) # Input size for LSTM training\r\n",
        "\r\n",
        "\r\n",
        "TRAINING_SOURCE = '/content/data/public/training/'\r\n",
        "VALID_SOURCE = '/content/data-public/data-validation/'\r\n",
        "MODE = 'TRAINING'\r\n",
        "MODEL_OUTPUT = '/content/gdrive/My Drive/Breath-Data/data/model_output'\r\n",
        "\r\n",
        "RUN_TITLE = \"26082021-Residual\"\r\n",
        "\r\n",
        "#####################################\r\n",
        "# Set the config for training\r\n",
        "\r\n",
        "# BATCH_SIZE1 = 64\r\n",
        "# # LIST_LABELS = ['normal', 'deep', 'heavy', 'other']\r\n",
        "# LIST_LABELS1 = ['normal', 'deep', 'heavy', 'other']\r\n",
        "# N_CLASSES1 = len(LIST_LABELS1)\r\n",
        "# N_EPOCHS1 = 10\r\n",
        "\r\n",
        "\r\n",
        "# # INPUT_SIZE = (32, 140, 1) # Input size for CNN training\r\n",
        "# INPUT_SIZE1 = (32, 251) # Input size for LSTM training\r\n",
        "\r\n",
        "\r\n",
        "# TRAINING_SOURCE1 = '/content/gdrive/My Drive/Breath-Data/data/training/'\r\n",
        "# VALID_SOURCE1 = '/content/gdrive/My Drive/Breath-Data/data/validation/'\r\n",
        "# MODE1 = 'TRAINING'\r\n",
        "# MODEL_OUTPUT1 = '/content/gdrive/My Drive/Breath-Data/data/model_output'\r\n",
        "\r\n",
        "# RUN_TITLE1 = \"20210706-Residual\""
      ],
      "outputs": [],
      "metadata": {
        "id": "OXjsIqZnw5XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data"
      ],
      "metadata": {
        "id": "lWxPI7RDvczx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Generate data for training\r\n",
        "# try:\r\n",
        "train_generator = BreathDataTrainingGenerator(\r\n",
        "          TRAINING_SOURCE,\r\n",
        "          list_labels=LIST_LABELS,\r\n",
        "          batch_size=BATCH_SIZE,\r\n",
        "          dim=INPUT_SIZE,\r\n",
        "          shuffle=False)\r\n",
        "# except Exception as e:\r\n",
        "#   pass\r\n",
        "\r\n",
        "N_TRAIN_SAMPLES = len(train_generator.wavs)\r\n",
        "print(\"Train samples: {}\".format(N_TRAIN_SAMPLES))\r\n",
        "\r\n",
        "validation_generator = BreathDataTrainingGenerator(\r\n",
        "        VALID_SOURCE,\r\n",
        "        list_labels=LIST_LABELS,\r\n",
        "        batch_size=BATCH_SIZE,\r\n",
        "        dim=INPUT_SIZE,\r\n",
        "        shuffle=False)\r\n",
        "N_VALID_SAMPLES = len(validation_generator.wavs)\r\n",
        "print(\"Validation samples: {}\".format(N_VALID_SAMPLES))\r\n",
        "\r\n",
        "##################################################\r\n",
        "# Generate data for training\r\n",
        "# try:\r\n",
        "# train_generator1 = BreathDataTrainingGenerator(\r\n",
        "#           TRAINING_SOURCE1,\r\n",
        "#           list_labels=LIST_LABELS1,\r\n",
        "#           batch_size=BATCH_SIZE1,\r\n",
        "#           dim=INPUT_SIZE1,\r\n",
        "#           shuffle=False)\r\n",
        "# # except Exception as e:\r\n",
        "# #   pass\r\n",
        "\r\n",
        "# N_TRAIN_SAMPLES1 = len(train_generator1.wavs)\r\n",
        "# print(\"Train samples: {}\".format(N_TRAIN_SAMPLES1))\r\n",
        "\r\n",
        "# validation_generator1 = BreathDataTrainingGenerator(\r\n",
        "#         VALID_SOURCE1,\r\n",
        "#         list_labels=LIST_LABELS1,\r\n",
        "#         batch_size=BATCH_SIZE1,\r\n",
        "#         dim=INPUT_SIZE1,\r\n",
        "#         shuffle=False)\r\n",
        "# N_VALID_SAMPLES1 = len(validation_generator1.wavs)\r\n",
        "# print(\"Validation samples: {}\".format(N_VALID_SAMPLES1))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-5hNCISvnSY",
        "outputId": "4a4d774a-1352-4e5b-8c9d-8fd88db1e78e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) CNN"
      ],
      "metadata": {
        "id": "4C06ehw0bH7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.callbacks import Callback,ModelCheckpoint\r\n",
        "from tensorflow.keras.models import Sequential,load_model\r\n",
        "from tensorflow.keras.layers import Dense, Dropout\r\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "\r\n",
        "def get_recall(y_true, y_pred): #taken from old keras source code\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    # precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    # f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\r\n",
        "    return recall\r\n",
        "\r\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\r\n",
        "    return f1_val\r\n",
        "\r\n",
        "# import tensorflow_addons as tfa\r\n",
        "class CNN_MODEL(object):\r\n",
        "    @staticmethod\r\n",
        "    def build(input_shape, classes, learning_rate):\r\n",
        "        # model = Sequential()\r\n",
        "        # model.add(CuLSTM(units=128, return_sequences=True, input_shape=data_input_shape))\r\n",
        "        # model.add(CuLSTM(units=128,  return_sequences=False))\r\n",
        "        # model.add(Dense(units=64, activation=\"relu\"))\r\n",
        "        # model.add(Dense(units=classes, activation=\"softmax\"))\r\n",
        "        # # Keras optimizer defaults:\r\n",
        "        # # Adam   : lr=0.001, beta_1=0.9,  beta_2=0.999, epsilon=1e-8, decay=0.\r\n",
        "        # # RMSprop: lr=0.001, rho=0.9,                   epsilon=1e-8, decay=0.\r\n",
        "        # # SGD    : lr=0.01,  momentum=0.,                             decay=0.\r\n",
        "        # opt = Adam(lr=learning_rate)\r\n",
        "        # model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\r\n",
        "        # return model\r\n",
        "\r\n",
        "\r\n",
        "        model = Sequential()\r\n",
        "\r\n",
        "        model.add(Conv2D(80, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform',\r\n",
        "                      input_shape=input_shape))\r\n",
        "        model.add(LeakyReLU())\r\n",
        "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\r\n",
        "\r\n",
        "        model.add(Conv2D(160, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform'))\r\n",
        "        model.add(LeakyReLU())\r\n",
        "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\r\n",
        "\r\n",
        "        model.add(Conv2D(240, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform'))\r\n",
        "        model.add(LeakyReLU())\r\n",
        "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\r\n",
        "\r\n",
        "        model.add(Flatten())\r\n",
        "        model.add(Dropout(0.5))\r\n",
        "\r\n",
        "        model.add(Dense(classes, kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform'))\r\n",
        "        model.add(Activation('softmax'))\r\n",
        "\r\n",
        "        optimizer = tf.keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\r\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1])\r\n",
        "      \r\n",
        "        return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "uraa7Q54i2WS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = CNN_MODEL.build(input_shape=INPUT_SIZE, classes=N_CLASSES, learning_rate=0.001)\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Checkpoint\r\n",
        "if not os.path.exists(MODEL_OUTPUT):\r\n",
        "    os.makedirs(MODEL_OUTPUT)\r\n",
        "\r\n",
        "filepath= os.path.join(MODEL_OUTPUT, RUN_TITLE + \"CNN-model-{epoch:02d}-{accuracy:.2f}.hdf5\") \r\n",
        "# filepath=\"./model_output/LSTM-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\r\n",
        "\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=False, mode='max')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "\r\n",
        "# Start training\r\n",
        "history = model.fit_generator(\r\n",
        "        train_generator,\r\n",
        "        steps_per_epoch= N_TRAIN_SAMPLES // BATCH_SIZE,\r\n",
        "        initial_epoch=0,\r\n",
        "        epochs=N_EPOCHS,\r\n",
        "        validation_data=validation_generator,\r\n",
        "        validation_steps=N_VALID_SAMPLES // BATCH_SIZE,\r\n",
        "        callbacks=callbacks_list,\r\n",
        "        use_multiprocessing=True,\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCsXcCVwkPMS",
        "outputId": "3c649147-e4cb-4815-c2f8-0b4da5d68d42"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import keras\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.savefig('/content/gdrive/My Drive/Breath-Data/cnn-breath-accuracy-mfcc-rnnoise-12082021.pdf')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "p1W8ceSAcvtQ",
        "outputId": "6f461c8a-1717-46d4-c4b9-53066334f732"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.savefig('/content/gdrive/My Drive/Breath-Data/cnn-breath-loss-mfcc-rnnoise-12082021.pdf')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "NxPXy99wcyuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2de13d27-876d-4e28-ba43-b44a16919874"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "n_batches = len(validation_generator)\r\n",
        "\r\n",
        "confusion_matrix(\r\n",
        "    np.concatenate([np.argmax(validation_generator[i][1], axis=1) for i in range(n_batches)]),    \r\n",
        "    np.argmax(model.predict_generator(validation_generator, steps=n_batches), axis=1) \r\n",
        ")\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibpL_K13czPn",
        "outputId": "4652ccd9-1408-4820-da65-3e6f7fd389d1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "n_batches = len(validation_generator)\r\n",
        "\r\n",
        "classification_report(\r\n",
        "    np.concatenate([np.argmax(validation_generator[i][1], axis=1) for i in range(n_batches)]),    \r\n",
        "    np.argmax(model.predict_generator(validation_generator, steps=n_batches), axis=1) \r\n",
        ")\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "PUeBPV0sczR8",
        "outputId": "94b1c8d7-eb40-4131-c126-ae0883d26b44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) BiLSTM"
      ],
      "metadata": {
        "id": "bPPqLgTDbOAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.callbacks import Callback,ModelCheckpoint\r\n",
        "from tensorflow.keras.models import Sequential,load_model\r\n",
        "from tensorflow.keras.layers import Dense, Dropout\r\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "\r\n",
        "def get_recall(y_true, y_pred): #taken from old keras source code\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    # precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    # f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\r\n",
        "    return recall\r\n",
        "\r\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\r\n",
        "    return f1_val\r\n",
        "\r\n",
        "class LSTM_MODEL(object):\r\n",
        "    @staticmethod\r\n",
        "    def build_simple_lstm(data_input_shape, classes, learning_rate):\r\n",
        "        model = Sequential()\r\n",
        "        model.add(CuLSTM(units=128, return_sequences=True, input_shape=data_input_shape))\r\n",
        "        model.add(CuLSTM(units=128,  return_sequences=False))\r\n",
        "        model.add(Dense(units=64, activation=\"relu\"))\r\n",
        "        model.add(Dense(units=classes, activation=\"softmax\"))\r\n",
        "        # Keras optimizer defaults:\r\n",
        "        # Adam   : lr=0.001, beta_1=0.9,  beta_2=0.999, epsilon=1e-8, decay=0.\r\n",
        "        # RMSprop: lr=0.001, rho=0.9,                   epsilon=1e-8, decay=0.\r\n",
        "        # SGD    : lr=0.01,  momentum=0.,                             decay=0.\r\n",
        "        opt = Adam(lr=learning_rate)\r\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), get_f1])\r\n",
        "        return model\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def build_bilstm(data_input_shape, classes, learning_rate):\r\n",
        "        model = Sequential()\r\n",
        "        model.add(Bidirectional(CuLSTM(128), input_shape=data_input_shape))\r\n",
        "        model.add(Dropout(0.5))\r\n",
        "        model.add(Dense(classes, activation='softmax'))\r\n",
        "        opt = Adam(lr=learning_rate)\r\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), get_f1])\r\n",
        "        # model.summary()\r\n",
        "\r\n",
        "        return model\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def build_residual_bilstm(data_input_shape, classes, learning_rate):\r\n",
        "        inp = Input(shape=data_input_shape)\r\n",
        "        # inp = TimeDistributed(Sequential())(inp)\r\n",
        "\r\n",
        "        # inp = SincConvFast_1(64, 251, sample_frequency)(inp)\r\n",
        "        # inp = normalize(inp)\r\n",
        "        # inp = LayerNormalization()(inp)\r\n",
        "        # inp = SincConvFast(64, 251, sample_frequency)(inp)\r\n",
        "        z1 = Bidirectional(CuLSTM(64, return_sequences=True))(inp)\r\n",
        "        z2 = Bidirectional(CuLSTM(units=64, return_sequences=True))(z1)\r\n",
        "        z3 = add([z1, z2])  # residual connection\r\n",
        "        z4 = Bidirectional(CuLSTM(units=64, return_sequences=True))(z3)\r\n",
        "        z5 = Bidirectional(CuLSTM(units=64, return_sequences=False))(z4)\r\n",
        "        z6 = add([z4, z5])  # residual connection    \r\n",
        "        z61 = Flatten()(z6)        \r\n",
        "        z7 = Dense(128, activation='relu')(z61)\r\n",
        "        z8 = Dropout(0.5)(z7)\r\n",
        "        # out = Dense(classes, activation='softmax')(z8)\r\n",
        "        out = Dense(classes, activation='softmax')(z8)\r\n",
        "        model = Model(inputs=[inp], outputs=out)\r\n",
        "        opt = Adam(lr=learning_rate)\r\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1])\r\n",
        "        # model.summary()\r\n",
        "        return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "pPYpWbzTbO6Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# INPUT_SIZE = (32, 251) # Input size for BiLSTM training\r\n",
        "\r\n",
        "model = LSTM_MODEL.build_residual_bilstm(data_input_shape=INPUT_SIZE, classes=N_CLASSES, learning_rate=0.001)\r\n",
        "\r\n",
        "filepath= os.path.join(MODEL_OUTPUT, RUN_TITLE + \"bi_lstm-{epoch:02d}-{accuracy:.2f}-{val_accuracy:.2f}.hdf5\") \r\n",
        "# filepath=\"./model_output/LSTM-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\r\n",
        "\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=False, mode='max')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Start training\r\n",
        "history = model.fit_generator(\r\n",
        "        train_generator,\r\n",
        "        steps_per_epoch= N_TRAIN_SAMPLES // BATCH_SIZE,\r\n",
        "        initial_epoch=0,\r\n",
        "        epochs=N_EPOCHS,\r\n",
        "        validation_data=validation_generator,\r\n",
        "        validation_steps=N_VALID_SAMPLES // BATCH_SIZE,\r\n",
        "        callbacks=callbacks_list,\r\n",
        "        use_multiprocessing=True,\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Zmpn69h3vu14",
        "outputId": "3aec7642-b582-4e19-fe1b-c44a8c32a9e4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import keras\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.savefig('/content/gdrive/My Drive/Breath-Data/bilstm-breath-accuracy-mfcc-14082021.pdf')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "rdkrO9SBl4IC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "0054125c-f833-42c6-81a3-3325da82854d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.savefig('/content/gdrive/My Drive/Breath-Data/bilstm-breath-loss-mfcc-13082021.pdf')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "XPzs23uMmPUT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d337f972-ebfa-4ad1-e7c8-5b8453a62674"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "n_batches = len(validation_generator)\r\n",
        "\r\n",
        "confusion_matrix(\r\n",
        "    np.concatenate([np.argmax(validation_generator[i][1], axis=1) for i in range(n_batches)]),    \r\n",
        "    np.argmax(model.predict_generator(validation_generator, steps=n_batches), axis=1) \r\n",
        ")\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "MiSiTyMH628Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992677df-26b3-476d-bf9e-e3f3f7e36751"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "n_batches = len(validation_generator)\r\n",
        "\r\n",
        "classification_report(\r\n",
        "    np.concatenate([np.argmax(validation_generator[i][1], axis=1) for i in range(n_batches)]),    \r\n",
        "    np.argmax(model.predict_generator(validation_generator, steps=n_batches), axis=1) \r\n",
        ")\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "hziy6CHKQcTZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "b124b2da-4e82-42f0-8372-83536660dcc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) LSTM"
      ],
      "metadata": {
        "id": "t00bDwftbRuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv3D\r\n",
        "from tensorflow.keras.layers import ConvLSTM2D\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\r\n",
        "from tensorflow.keras.preprocessing.image import load_img\r\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.callbacks import Callback,ModelCheckpoint\r\n",
        "from tensorflow.keras.models import Sequential,load_model\r\n",
        "from tensorflow.keras.layers import Dense, Dropout\r\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "\r\n",
        "def get_recall(y_true, y_pred): #taken from old keras source code\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    # precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    # f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\r\n",
        "    return recall\r\n",
        "\r\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\r\n",
        "    return f1_val\r\n",
        "\r\n",
        "\r\n",
        "# https://github.com/lvrysis/Audio-DNN-Classification/blob/master/Audio-DNN/Python/DeepCNN2DLSTM.py\r\n",
        "\r\n",
        "class LSTM_MODEL(object):\r\n",
        "    @staticmethod\r\n",
        "    def build_simple_lstm(data_input_shape, classes, learning_rate):\r\n",
        "        model = Sequential()\r\n",
        "        model.add(CuLSTM(units=128, return_sequences=True, input_shape=data_input_shape))\r\n",
        "        model.add(CuLSTM(units=128,  return_sequences=False))\r\n",
        "        model.add(Dense(units=64, activation=\"relu\"))\r\n",
        "        model.add(Dense(units=classes, activation=\"softmax\"))\r\n",
        "        # Keras optimizer defaults:\r\n",
        "        # Adam   : lr=0.001, beta_1=0.9,  beta_2=0.999, epsilon=1e-8, decay=0.\r\n",
        "        # RMSprop: lr=0.001, rho=0.9,                   epsilon=1e-8, decay=0.\r\n",
        "        # SGD    : lr=0.01,  momentum=0.,                             decay=0.\r\n",
        "        opt = Adam(lr=learning_rate)\r\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1])\r\n",
        "\r\n",
        "        model.summary()\r\n",
        "        return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "St9M8NBnbSsw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# INPUT_SIZE_2 = (32, 251, 1) # Input size for CNN training\r\n",
        "\r\n",
        "model = LSTM_MODEL.build_simple_lstm(data_input_shape=INPUT_SIZE, classes=N_CLASSES, learning_rate=0.001)\r\n",
        "\r\n",
        "filepath= os.path.join(MODEL_OUTPUT, RUN_TITLE + \"LSTM-weights-improvement_bi_lstm-{epoch:02d}-{accuracy:.2f}-{val_accuracy:.2f}.hdf5\")\r\n",
        "# filepath=\"./model_output/LSTM-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\r\n",
        "\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=False, mode='max')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "\r\n",
        "# Start training\r\n",
        "model.fit_generator(\r\n",
        "        train_generator,\r\n",
        "        steps_per_epoch= N_TRAIN_SAMPLES // BATCH_SIZE,\r\n",
        "        initial_epoch=0,\r\n",
        "        epochs=N_EPOCHS,\r\n",
        "        validation_data=validation_generator,\r\n",
        "        validation_steps=N_VALID_SAMPLES // BATCH_SIZE,\r\n",
        "        callbacks=callbacks_list,\r\n",
        "        # use_multiprocessing=True,\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et8eEe-cn2Xv",
        "outputId": "43d49cb0-b5a9-4e91-c1f3-f38ca3a1da72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import keras\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.savefig('/content/gdrive/My Drive/Breath-Data/lstm-public-accuracy-mfcc-rnnoise-12082021.pdf')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "1-FIkS9quhcM",
        "outputId": "ecae25a0-da91-49b4-b058-21b6d24c4983"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.savefig('/content/gdrive/My Drive/Breath-Data/lstm-public-loss-mfcc-rnnoise-12082021.pdf')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "XwmVBsjiu1gA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "n_batches = len(validation_generator)\r\n",
        "\r\n",
        "confusion_matrix(\r\n",
        "    np.concatenate([np.argmax(validation_generator[i][1], axis=1) for i in range(n_batches)]),    \r\n",
        "    np.argmax(model.predict_generator(validation_generator, steps=n_batches), axis=1) \r\n",
        ")\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCz4AOguvBR_",
        "outputId": "7c56f479-8b66-4ee0-bca7-8cd6d2a430c7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "n_batches = len(validation_generator)\r\n",
        "\r\n",
        "classification_report(\r\n",
        "    np.concatenate([np.argmax(validation_generator[i][1], axis=1) for i in range(n_batches)]),    \r\n",
        "    np.argmax(model.predict_generator(validation_generator, steps=n_batches), axis=1) \r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "4hBe9KpkvQEL",
        "outputId": "188f1fc4-cef4-4b5d-dcc6-df25207de704"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Number of steps corresponding to an epoch\r\n",
        "steps = 10\r\n",
        "predictions = model.predict_generator(validation_generator, steps=steps)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3PvnS-c-pDj",
        "outputId": "7632fe8b-7b6d-4b1b-843c-0d43dc69a129"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "confusion_matrix(\r\n",
        "    np.concatenate([np.argmax(validation_generator[i][1], axis=1) for i in range(300)]),    \r\n",
        "    np.argmax(model.predict_generator(validation_generator, steps=300), axis=1) \r\n",
        ")\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m7F5WWBt_Oyb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# label names\r\n",
        "labels = validation_generator.class_indices.keys()\r\n",
        "precisions, recall, f1_score, _ = metrics.precision_recall_fscore_support(val_trues, val_preds, labels=labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5hX_zfpr_Tx5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(precisions)\r\n",
        "print(recall)\r\n",
        "print(f1_score)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BZnVHer_ZvQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Multi-task - BiLSTM or CNN: detect + classify"
      ],
      "metadata": {
        "id": "mdAC4sUkbU3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Data Multi Output"
      ],
      "metadata": {
        "id": "8oiqwJklQdu4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Generate data for training\r\n",
        "# try:\r\n",
        "train_generator_multi = BreathDataTrainingGeneratorMulti(\r\n",
        "          TRAINING_SOURCE,\r\n",
        "          list_labels=LIST_LABELS,\r\n",
        "          batch_size=BATCH_SIZE,\r\n",
        "          dim=INPUT_SIZE,\r\n",
        "          shuffle=False)\r\n",
        "# except Exception as e:\r\n",
        "#   pass\r\n",
        "\r\n",
        "N_TRAIN_SAMPLES = len(train_generator_multi.wavs)\r\n",
        "print(\"Train samples: {}\".format(N_TRAIN_SAMPLES))\r\n",
        "\r\n",
        "validation_generator_multi = BreathDataTrainingGeneratorMulti(\r\n",
        "        VALID_SOURCE,\r\n",
        "        list_labels=LIST_LABELS,\r\n",
        "        batch_size=BATCH_SIZE,\r\n",
        "        dim=INPUT_SIZE,\r\n",
        "        shuffle=False)\r\n",
        "N_VALID_SAMPLES = len(validation_generator_multi.wavs)\r\n",
        "print(\"Validation samples: {}\".format(N_VALID_SAMPLES))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvgVRs_HNXQ-",
        "outputId": "eec4b769-5cb3-4375-ff8f-1c627819e4f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Init Model"
      ],
      "metadata": {
        "id": "9m5p6AJIQhwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv3D\r\n",
        "from tensorflow.keras.layers import ConvLSTM2D\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\r\n",
        "from tensorflow.keras.preprocessing.image import load_img\r\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.callbacks import Callback,ModelCheckpoint\r\n",
        "from tensorflow.keras.models import Sequential,load_model\r\n",
        "from tensorflow.keras.layers import Dense, Dropout\r\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "\r\n",
        "def get_recall(y_true, y_pred): #taken from old keras source code\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    # precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    # f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\r\n",
        "    return recall\r\n",
        "\r\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\r\n",
        "    return f1_val\r\n",
        "\r\n",
        "def get_trainable_model_bilstm(data_input_shape, classes, learning_rate):\r\n",
        "    inp1 = Input(shape=data_input_shape) # detect\r\n",
        "    inp2 = Input(shape=data_input_shape) # classify\r\n",
        "\r\n",
        "    z1 = Bidirectional(CuLSTM(128, return_sequences=True))(inp1)\r\n",
        "    z2 = Bidirectional(CuLSTM(units=128, return_sequences=True))(z1)\r\n",
        "    z3 = add([z1, z2])  # residual connection\r\n",
        "    z4 = Bidirectional(CuLSTM(units=128, return_sequences=True))(z3)\r\n",
        "    z5 = Bidirectional(CuLSTM(units=128, return_sequences=False))(z4)\r\n",
        "    z6 = add([z4, z5])  # residual connection    \r\n",
        "    z61 = Flatten()(z6) \r\n",
        "    z7 = Dense(256, activation='relu')(z61)\r\n",
        "    z8 = Dropout(0.5)(z7)\r\n",
        "    # out = Dense(classes, activation='softmax')(z8)\r\n",
        "    out1 = Dense(3, activation='softmax', name='output_1')(z8)\r\n",
        "    out2 = Dense(2, activation='softmax', name='output_2')(z8)\r\n",
        "\r\n",
        "    model = Model(inputs=inp1, outputs=[out1, out2])\r\n",
        "    opt = Adam(lr=learning_rate)\r\n",
        "    losses = {\r\n",
        "          \"output_1\": \"categorical_crossentropy\",\r\n",
        "          \"output_2\": \"categorical_crossentropy\"\r\n",
        "    }\r\n",
        "    metricss = {\r\n",
        "        \"output_1\": ['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1],\r\n",
        "        \"output_2\": ['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1]\r\n",
        "    }\r\n",
        "    model.compile(loss=losses, optimizer=opt, metrics=metricss)\r\n",
        "    model.summary()\r\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "vlClcfi5bbRG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow\r\n",
        "\r\n",
        "def get_trainable_model_cnn(data_input_shape, classes, learning_rate):\r\n",
        "    inp = Input(shape=data_input_shape) # detect\r\n",
        "\r\n",
        "    c1 = Conv2D(80, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform')(inp)\r\n",
        "    c2 = LeakyReLU()(c1)\r\n",
        "    c3 = MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same')(c2)\r\n",
        "\r\n",
        "    c4 = Conv2D(160, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform')(c3)\r\n",
        "    c5 = LeakyReLU()(c4)\r\n",
        "    c6 = MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same')(c5)\r\n",
        "\r\n",
        "    c7 = Conv2D(240, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform')(c6)\r\n",
        "    c8 = LeakyReLU()(c7)\r\n",
        "    c9 = MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same')(c8)\r\n",
        "\r\n",
        "    c10 = Flatten()(c9)\r\n",
        "    c11 = Dropout(0.5)(c10)\r\n",
        "\r\n",
        "    out1 = Dense(4, activation='softmax', name='output_1')(c11)\r\n",
        "    out2 = Dense(2, activation='softmax', name='output_2')(c11)\r\n",
        "\r\n",
        "    model = Model(inputs=inp, outputs=[out1, out2])\r\n",
        "    opt = Adam(lr=learning_rate)\r\n",
        "    losses = {\r\n",
        "          \"output_1\": \"categorical_crossentropy\",\r\n",
        "          \"output_2\": \"categorical_crossentropy\"\r\n",
        "    }\r\n",
        "    metricss = {\r\n",
        "        \"output_1\": ['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1],\r\n",
        "        \"output_2\": ['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1]\r\n",
        "    }\r\n",
        "    model.compile(loss=losses, optimizer=opt, metrics=metricss)\r\n",
        "    model.summary()\r\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "FX5ym7SMJaBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow\r\n",
        "\r\n",
        "def get_trainable_model_lstm(data_input_shape, classes, learning_rate):\r\n",
        "    inp = Input(shape=data_input_shape)\r\n",
        "\r\n",
        "    l1 = CuLSTM(units=128, return_sequences=True)(inp)\r\n",
        "    l2 = CuLSTM(units=128,  return_sequences=False)(l1)\r\n",
        "    l3 = Dense(units=64, activation=\"relu\")(l2)\r\n",
        "\r\n",
        "    out1 = Dense(4, activation='softmax', name='output_1')(l3)\r\n",
        "    out2 = Dense(2, activation='softmax', name='output_2')(l3)\r\n",
        "\r\n",
        "    model = Model(inputs=inp, outputs=[out1, out2])\r\n",
        "    opt = Adam(lr=learning_rate)\r\n",
        "    losses = {\r\n",
        "          \"output_1\": \"categorical_crossentropy\",\r\n",
        "          \"output_2\": \"categorical_crossentropy\"\r\n",
        "    }\r\n",
        "    metricss = {\r\n",
        "        \"output_1\": ['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1],\r\n",
        "        \"output_2\": ['accuracy', tf.keras.metrics.Precision(), get_recall, get_f1]\r\n",
        "    }\r\n",
        "    model.compile(loss=losses, optimizer=opt, metrics=metricss)\r\n",
        "    model.summary()\r\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "bli2nW87jkxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = get_trainable_model_bilstm(data_input_shape=INPUT_SIZE, classes=N_CLASSES, learning_rate=0.001)\r\n",
        "# model = get_trainable_model_cnn(data_input_shape=INPUT_SIZE, classes=N_CLASSES, learning_rate=0.001)\r\n",
        "# model = get_trainable_model_lstm(data_input_shape=INPUT_SIZE, classes=N_CLASSES, learning_rate=0.001)\r\n",
        "\r\n",
        "filepath= os.path.join(MODEL_OUTPUT, RUN_TITLE + \"bi_lstm-{epoch:02d}.hdf5\")\r\n",
        "# filepath=\"./model_output/LSTM-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\r\n",
        "\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=False, mode='max')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "\r\n",
        "# Start training\r\n",
        "model.fit_generator(\r\n",
        "        train_generator_multi,\r\n",
        "        steps_per_epoch= N_TRAIN_SAMPLES // BATCH_SIZE,\r\n",
        "        initial_epoch=0,\r\n",
        "        epochs=N_EPOCHS,\r\n",
        "        validation_data=validation_generator_multi,\r\n",
        "        validation_steps=N_VALID_SAMPLES // BATCH_SIZE,\r\n",
        "        callbacks=callbacks_list,\r\n",
        "        # use_multiprocessing=True,\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "yUXVHh-y3O8b",
        "outputId": "ce6e8c17-1faf-4de4-9a48-488ddb4b1c4a"
      }
    }
  ]
}